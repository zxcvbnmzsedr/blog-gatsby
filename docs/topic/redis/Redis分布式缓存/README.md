---
title: Redis分布式缓存
date: 2022-04-28 14:17
permalink: /topic/redis/Redis%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98
topic: 
  - topic
tags: null
categories: 
  - topic
  - redis
  - Redis分布式缓存
---
> 旁路: 写时:更新DB，删除cache； 读时miss后查DB回写cache,用于高一致性的场景​
>
> 穿透: 写时:hit则更新DB和cache，miss仅更新DB；读时:miss则查询DB并回写，适用于冷热分区​
>
> 异步: 写时:只更新cache，异步更新DB，读时:miss后查询DB并回写，适用于高频写​
>
> 兜底: 写时:直接写DB，读时:读DB失败后读cache/成功后回写cache，适用于高可用场景​
>
> 只读: 写时: 更新DB，读时，读cache，其他异步方式更新cache，适用于100%命中率场景,最终一致场景​
>
> 回源: 写时: 写DB，读时: 读DB， 适用于缓存的降级时期。
>

　　‍

　　用redis作为最基本的功能，缓存mysql中的数据，加快响应速度

# 双写一致性

　　双写一致性，是redis做为缓存的时候一道非常经典的问题

　　因为无论先操作db还是cache，都会有各自的问题，根本原因是cache和db的更新不是一个原子操作，因此总会有不一致的问题。

　　一致性可以分为三个等级:

1. 强一致性

   + 这种一致性级别是最符合用户直觉的，它要求系统写入什么，读出来的也会是什么，用户体验好，但实现起来往往对系统的性能影响大
2. 弱一致性

   + 这种一致性级别约束了系统在写入成功后，不承诺立即可以读到写入的值，也不承诺多久之后数据能够达到一致，但会尽可能地保证到某个时间级别（比如秒级别）后，数据能够达到一致状态
3. 最终一致性

   + 最终一致性是弱一致性的一个特例，系统会保证在一定时间内，能够达到一个数据一致的状态。这里之所以将最终一致性单独提出来，是因为它是弱一致性中非常推崇的一种一致性模型，也是业界在大型分布式系统的数据一致性上比较推崇的模型

# 缓存更新策略

## 最常见的策略

　　![img](https://www.shiyitopo.tech/uPic/db-and-cache-01-01.jpg)

#### 优点剖析

#### 1. “先淘汰缓存，再写数据库” 合理

　　为什么说这也算优点呢？试想一下

　　如果把写流程改一下：**先更新缓存，再更新DB**。 如果我们更新缓存成功，而更新数据库失败，就会导致缓存中的数据是错误的，而我们大部分的业务一般能忍受数据延迟，但是数据错误这是无法接受的，所以先淘汰缓存是比较合理的。 如果把写流程改一下：**不删缓存，先更新DB，再更新缓存**。 如果我们更新DB成功，而更新缓存失败，则会导致缓存中就会一直是旧的数据（也算是一种错误数据），所以先淘汰缓存是比较合理的。

#### 2. 异步刷新，补缺补漏

　　在很多业务场景中，缓存只是辅助，**所以在很多业务中，缓存的读写失败不会影响主流程**，啥意思呢？就是说很多情况下，即使操作缓存失败（比如步骤1.1中的’DEL缓存失败’），程序还是会继续往下走（继续步骤1.2 更新数据库)，所以这个时候异步刷新就能在一定程度上，对1.1的失败进行错误数据的修补

#### 缺点剖析

#### 1. 容灾不足

> 在分布式领域，“Everything will fails”，任何可能出现问题的地方都会出现问题
>

　　我们来分析一下写流程，第一步，’DEL缓存失败’怎么办？流程是否还继续走？如果继续执行，那么从’更新完DB’到异步’刷新缓存’缓存期间，数据处于滞后状态。而且如果缓存处于不可写状态，那么异步刷新那步也可能会失败，那缓存就会长期处于旧数据，问题就比较严重了

　　如果A线程更新数据库内容失败，导致B线程请求再次访问缓存时，发现redis里面没数据，缓存缺失，再去读取mysql时， 从数据库中读取到旧值

#### 2. 并发问题

　　**写写并发：**试想一下，同时有多个服务器的多个线程进行’步骤1.2更新DB’，更新DB完成之后，它们就要进行异步刷缓存，我们都知道多服务器的异步操作，是无法保证顺序的，所以后面的刷新操作存在相互覆盖的并发问题，**也就是说，存在先更新的DB操作，反而很晚才去刷新缓存**，那这个时候，数据也是错的

　　**读写并发：**再试想一下，服务器A在进行’读操作’，，在A服务器刚完成2.2时，服务器B在进行’写操作’，假设B服务器1.3完成之后，服务器A的1.3才被执行，这个时候就相当于更新前的老数据写入缓存，最终数据还是错的

#### 方案总结

　　适合大部分的业务场景，很多人都在用，香还是很香的，实现起来也简单。
**适合使用的场景：并发量、一致性要求都不是很高的情况**。
我觉得这个方案有一个比较大的缺陷在于**刷新缓存有可能会失败，而失败之后缓存中数据就一直会处于错误状态，所以它并不能保证数据的最终一致性**

　　为了保证“数据最终一致性”，我们引入binlog，通过解析binlog来刷新缓存，这样即使刷新失败，依然可以进行日志回放，再次刷新缓存

　　![img](https://www.shiyitopo.tech/uPic/db-and-cache-02-01.jpg)

### 写流程：

　　第一步先删除缓存，删除之后再更新DB，我们监听从库(资源少的话主库也ok)的binlog，通过分析binlog我们解析出需要需要刷新的数据，然后读主库把最新的数据写入缓存。

> 这里需要提一下：最后刷新前的读主库或者读从库，甚至不读库直接通过binlog解析出需要的数据都是ok的，这由业务决定，**比如刷新的数据只是表的一行，那直接通过binlog就完全能解析出来；然而如果需要刷新的数据来自多行，多张表，甚至多个库的话，那就需要读主库或是从库才行**
>

### 读流程：

　　第一步先读缓存，如果缓存没读到，则去读DB，之后再异步将数据刷回缓存

## 方案分析

### 优点剖析

#### 1. 容灾

　　写步骤1.4或1.5 如果失败，可以进行日志回放，再次重试。
无论步骤1.1是否删除成功，后续的刷新操作是有保证的

> 妈耶，怎么就一个优点，讲道理这个其实很常用的，那我们再来看看缺点
>

### 缺点剖析

> 分析缺点之前，我们先来看一下知识点
>
> 1. 对于同一张表的同一条记录的更新，Databus会以串行形式的通知下游服务，也就是说，只有当我们正确返回后，它才会推送该记录的下一次更新。
> 2. 对于同一张表的不同记录的更新， Databus会以事件时间为顺序的通知下游服务，但并不会等待我们返回后才推送下一条，也就是说它是非串行的。
> 3. 对于不同表，根据其下游的消费速度，不同表之间没有明确的时间顺序。
>

#### 1. 只适合简单业务，复杂业务容易发生并发问题

　　这里先来解释一下这里说的“简单业务”是啥意思？

> 简单业务：每次需要刷新的数据，都来自**单表单行**。
>

　　为什么复杂业务就不行呢？我举个例子
我们假设 **一个订单 = A表信息 + B表信息**

　　![img](https://www.shiyitopo.tech/uPic/db-and-cache-02-02.jpg)

　　由于A表先变化，经过1，2，3步后，线程1获取了A’B （A表是新数据，B表的老数据），当线程1还没来得及刷新缓存时，并发发生了：

　　此时，B表发生了更新，经过4，5，6，7将最新的数据A’B’写入缓存，此时此刻缓存数据是符合要求的。

　　但是，后来线程1进行了第8步，将A’B写入数据，使得缓存最终结果 与 DB 不一致。

##### 缺点1的改进

- **针对单库多表单次更新的改进：利用事务**

　　![](https://www.shiyitopo.tech/uPic/db-and-cache-02-03.jpg)

　　当AB表的更新发生在一个事务内时，不管线程1、线程2如何读取，他们都能获取两张表的最新数据，所以刷新缓存的数据都是符合要求的。

> 但是这种方案具有局限性：那就是只对单次更新有效，或者说更新频率低的情况下才适应，比如我们并发的单独更新C表，并发问题依然会发生。
>

　　所以**这种方案只针对多表单次更新的情况**。

- **针对多表多次更新的改进：增量更新**

　　![img](https://www.shiyitopo.tech/uPic/db-and-cache-02-04.jpg)

　　每张表的更新，在同步缓存时，只获取该表的字段覆盖缓存。

　　这样，线程1，线程2总能获取对应表最新的字段，而且Databus对于同表同行会以串行的形式通知下游，所以能保证缓存的最终一致性。

> 这里有一点需要提一下：更新“某张表多行记录“时，这个操作要在一个事务内，不然并发问题依然存在，正如前面分析的
>

#### 2. 依然是并发问题

　　即使对于**缺点1**我们提出了改进方案，虽然它解决了部分问题，但在极端场景下依然存在并发问题。
这个场景，就是**缓存中没有数据**的情况：

- **读的时候，缓存中的数据已失效，此时又发生了更新**
- **数据更新的时候，缓存中的数据已失效，此时又发生了更新**

　　这个时候，我们在上面提到的“增量更新”就不起作用了，我们需要读取**所有的表**来拼凑出初始数据，那这个时候又涉及到**读所有表的操作**了，那我们在**缺点1**中提到的并发问题会再次发生

## 方案总结

　　**适合使用的场景：业务简单，读写QPS比较低的情况**。
今天这个方案呢，优缺点都比较明显，binlog用来刷新缓存是一个很棒的选择，它天然的顺序性用来做同步操作很具有优势；其实**它的并发问题来自于Canal 或 Databus。拿Databus来说，由于不同行、表、库的binlog的消费并不是时间串行的**，那怎么解决这个问题呢

> 强一致性，包含两种含义：
>
> - 缓存和DB数据一致
> - 缓存中没有数据（或者说：不会去读缓存中的老版本数据）
>

　　首先我们来分析一下，既然已经实现了“**最终一致性**”，那它和“**强一致性**”的区别是什么呢？没错，就是“**时间差**”，所以：

> “**最终一致性方案**” + “**时间差**” = “**强一致性方案**”
>

　　那我们的工作呢，就是加上时间差，实现方式：**我们加一个缓存，将近期被修改的数据进行标记锁定。读的时候，标记锁定的数据强行走DB，没锁定的数据，先走缓存**

　　![img](https://www.shiyitopo.tech/uPic/db-and-cache-04-01.jpg)

### 写流程：

　　我们把修改的数据通过Cache_0标记“正在被修改”，如果标记成功，则继续往下走；**那如果标记失败，则要放弃这次修改。**

> 何为标记锁定呢？比如你可以设定一个有效期为10S的key，Key存在即为锁定。一般来说10S对于后面的同步操作来说基本是够了~
>

　　**如果说，还想更严谨一点，怕DB主从延迟太久、MQ延迟太久，或Databus监听的从库挂机之类的情况，我们可以考虑增加一个监控定时任务**。
比如我们增加一个时间间隔2S的worker的去对比以下两个数据：

- **时间1： 最后修改数据库的时间**
  **VS**
- **时间2： 最后由更新引起的’MQ刷新缓存对应数据的实际更新数据库’的时间**

> 数据1： 可由步骤1.1获得，并存储
> 数据2： 需要由binlog中解析获得，需要透传到MQ，这样后面就能存储了
> 这里提一下：如果多库的情况的话，存储这两个key需要与库一一对应
>

　　如果 时间1 VS 时间2 相差超过5S，那我们就自动把相应的缓存分片读降级。

### 读流程：

　　先读Cache_0，看看要读的数据是否被标记，如果被标记，则直接读主库；

## 方案分析

### 优点剖析

#### 1. 容灾完善

　　我们一步一步来分析：

###### 写流程容灾分析

- **写1.1 标记失败**：没关系，放弃整个更新操作
- **写1.3 DEL缓存失败**：没关系，后面会覆盖
- **写1.5 写MQ失败**：没关系，Databus或Canal都会重试
- **消费MQ的：1.6 || 1.7 失败**：没关系，重新消费即可

###### 读流程容灾分析

- **读2.1 读Cache_0失败**：没关系，直接读主库
- **读2.3 异步写MQ失败**：没关系，缓存为空，是OK的，下次还读库就好了

#### 2. 无并发问题

　　这个方案让“读库 + 刷缓存”的操作串行化，这就不存在老数据覆盖新数据的并发问题了

### 缺点剖析

#### 1. 增加Cache_0强依赖

　　这个其实有点没办法，你要强一致性，必然要牺牲一些的。
但是呢，你这个可以吧Cache_0设计成多机器多分片，这样的话，即使部分分片挂了，也只有小部分流量透过Cache直接打到DB上，这是完全是可接受的

#### 2. 复杂度是比较高的

　　涉及到Databus、MQ、定时任务等等组件，实现起来复杂度还是有的

## 方案总结

　　OK，到此呢，我们已经实现了**“数据库和缓存强一致性”**，这个系列就先这样啦，等我学到了更好的方案，再来分享~

　　‍

　　https://blog.kido.site/2018/11/24/db-and-cache-preface/
